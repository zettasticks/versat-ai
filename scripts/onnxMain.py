import sys
import os
import glob

# Missing split_complex_to_pairs

from versatDefs import *
from memoryAllocator import CalculateMemoryAllocations
from onnxAddOutputsToIntermediate import AddOutputsToEachNode
from onnxOperators import *
from copy import copy

from onnx import shape_inference
from skl2onnx.helpers.onnx_helper import enumerate_model_node_outputs
from skl2onnx.helpers.onnx_helper import select_model_inputs_outputs
from skl2onnx.helpers.onnx_helper import save_onnx_model
from functools import reduce
from pprint import pprint

import struct
import numpy as np
import onnx

import onnxruntime as ort

from onnx import __version__, IR_VERSION
from onnx.defs import onnx_opset_version
from onnx import numpy_helper

import matplotlib.pyplot as plt


# Nodes have either inputs from other nodes or initializers, which are the constant values embedded in the model.
# If a given node input is a tensor then it is not an input and vice versa.
def GetTensor(model, tensorName):
    for tensor in model.graph.initializer:
        if tensor.name == tensorName:
            return tensor


# TODO: Need to properly handle different types and stuff like that. We might even need to handle padding
def TensorSize(tensor: list[int]):
    return reduce(lambda x, y: x * y, tensor) * 4  # 4 because size of float


def GetValueForDim(dim):
    if dim.WhichOneof("value") == "dim_value":
        return dim.dim_value
    else:
        return dim.dim_param


def GetShape(model, name):
    assert name  # Make sure that we got a name, onnx models contain a lot of members that contain optional names, which might work for some models and not others. Care

    for value in model.graph.output:
        if value.name == name:
            return [GetValueForDim(x) for x in value.type.tensor_type.shape.dim]
    for value in model.graph.input:
        if value.name == name:
            return [GetValueForDim(x) for x in value.type.tensor_type.shape.dim]
    for value in model.graph.value_info:
        if value.name == name:
            return [GetValueForDim(x) for x in value.type.tensor_type.shape.dim]
    for value in model.graph.initializer:
        if value.name == name:
            return [int(x) for x in value.dims]

    # NOTE: We want this function to be able to obtain all the shapes from a given name.
    #       Need to care with the fact that some onnx names are optional. All the names that this function check are mandatory
    #       so this function works fine, just need to make sure that the name that we receive as input actually exists,
    #       and that we did not get it from a optional location, since different graphs might not implement them and this will fail.

    print(model.graph)
    print(f"Could not find shape for {name}")
    assert False


def CalculateOffsetFromSize(sizes: list[int]):
    offset = 0
    result = [offset]
    for size in sizes[:-1]:
        offset += size
        result.append(offset)

    totalSize = offset + sizes[-1]
    return result, totalSize


def EndianessToStructArg(endianess: Endianess):
    endianArg = ""
    if endianess == Endianess.NATIVE:
        endianArg = "@"
    elif endianess == Endianess.LITTLE_ENDIAN:
        endianArg = "<"
    elif endianess == Endianess.BIG_ENDIAN:
        endianArg = ">"
    else:
        assert False

    return endianArg


def PackArrayNoHeader(array, endianess: Endianess = Endianess.NATIVE):
    endianArg = EndianessToStructArg(endianess)

    dtype = array.dtype
    formatArg = "f"
    if dtype == np.int64:
        formatArg = "q"

    data = struct.Struct(f"{endianArg}{formatArg}")
    dataContent = bytearray()
    for x in np.nditer(array):
        dataContent += data.pack(x)

    return dataContent


def PackMultipleArrays(arrayList, endianess: Endianess = Endianess.NATIVE):
    data = bytearray()
    offsets = []
    for array in arrayList:
        offsets.append(len(data))
        data += PackArrayNoHeader(array)

    return PackedArrays(data, offsets)


def IndexOfNodeThatProducesOutput(cModel, outputName):
    for index, op in enumerate(cModel.operations):
        if outputName == op.output:
            return index
    return None


def GenerateModelFromOnnxModel(onnxModel):
    shaped = shape_inference.infer_shapes(onnxModel)
    cModel = Model(shaped)

    # Need the shape of the inputs and output tensors

    inputNames = []
    for value in onnxModel.graph.input:
        shape = [GetValueForDim(x) for x in value.type.tensor_type.shape.dim]

        if not GetTensor(shaped, value.name):
            inputNames.append(Port(value.name, shape))
    cModel.modelInputs = inputNames

    # Extract all the data that we care about from the graph into a simpler struct for further processing.
    for node in onnxModel.graph.node:
        opType = node.op_type

        # TODO: Need to properly handle the operator not being registered
        operatorSpec = operatorNameToSpec[opType]
        attributesSpec = operatorSpec.attributesDict

        parsedAttributes = {}
        for attribute in node.attribute:
            attributeName = attribute.name

            spec = attributesSpec[attributeName]

            parsedAttribute = None
            if spec.attrType == OnnxAttributeType.INTEGER:
                parsedAttribute = InstantiatedAttribute(spec, int(attribute.i))
            elif spec.attrType == OnnxAttributeType.BOUNDED_INTEGER:
                parsedAttribute = InstantiatedAttribute(spec, int(attribute.i))
            elif spec.attrType == OnnxAttributeType.AXIS_LIST:
                parsedAttribute = InstantiatedAttribute(
                    spec, [int(x) for x in attribute.ints]
                )
            elif spec.attrType == OnnxAttributeType.AXIS_PAIR_LIST:
                parsedAttribute = InstantiatedAttribute(
                    spec, [int(x) for x in attribute.ints]
                )
            elif spec.attrType == OnnxAttributeType.INTEGER_LIST:
                parsedAttribute = InstantiatedAttribute(
                    spec, [int(x) for x in attribute.ints]
                )
            elif spec.attrType == OnnxAttributeType.BOUNDED_STRING:
                parsedAttribute = InstantiatedAttribute(
                    spec, attribute.s.decode("UTF-8")
                )
            else:
                assert False

            parsedAttributes[attribute.name] = parsedAttribute

        inputDimensions = []
        for name in node.input:
            shape = GetShape(shaped, name)
            inputDimensions.append(shape)
            tensor = GetTensor(onnxModel, name)
            if tensor:
                asNpArray = onnx.numpy_helper.to_array(tensor)
                cModel.initializers.append(asNpArray)

        outputDimensions = None
        for output in node.output:
            shape = GetShape(shaped, output)
            outputDimensions = shape

        dataSources = []
        for name in node.input:
            tensor = GetTensor(onnxModel, name)
            source = None
            if tensor:
                source = DataSource(DataSourceType.INITIALIZER, name)
            elif name in [x.name for x in cModel.modelInputs]:
                source = DataSource(DataSourceType.MODEL_INPUT, name)
            else:
                source = DataSource(DataSourceType.NODE_INPUT, name)
            dataSources.append(source)

        outputName = node.output[0]  # Can a node have more than one output?

        op = Operation(
            node.name,
            node.op_type,
            dataSources,
            outputName,
            inputDimensions,
            outputDimensions,
            parsedAttributes,
        )
        cModel.operations.append(op)

    # Calculate initializer position
    initializersSeen = 0
    for index, c in enumerate(cModel.operations):
        for source in c.inputs:
            if source.sourceType == DataSourceType.INITIALIZER:
                source.index = initializersSeen
                initializersSeen += 1
            elif source.sourceType == DataSourceType.MODEL_INPUT:
                for index, port in enumerate(cModel.modelInputs):
                    if port.name == source.name:
                        source.index = index
            else:
                source.index = IndexOfNodeThatProducesOutput(cModel, source.name)

    pprint(cModel)

    allInputsShapes = sum([port.shape for port in cModel.modelInputs], [])

    # TODO: Currently we force variable sizes to 1 in here
    mapped = {}
    forcedSize = 1

    for port in cModel.modelInputs:
        for i, name in enumerate(port.shape):
            if type(name) == str:
                port.shape[i] = mapped.get(name, forcedSize)
                mapped[name] = port.shape[i]

    for op in cModel.operations:
        for inputs in op.inputDimensions:
            for i, name in enumerate(inputs):
                if type(name) == str:
                    inputs[i] = mapped.get(name, forcedSize)
                    mapped[name] = inputs[i]

        for i, name in enumerate(op.outputDimensions):
            if type(name) == str:
                op.outputDimensions[i] = mapped.get(name, forcedSize)
                mapped[name] = op.outputDimensions[i]

    print(mapped.keys())

    for name in mapped.keys():
        print(name, allInputsShapes)
        if name in allInputsShapes:
            print(f"{name} is an input")
        else:
            print(f"{name} is not an input")

    CalculateMemoryAllocations(cModel)

    pprint(cModel)

    return cModel


def RunModel(model: Model, inputs):
    if not model.sess:
        model.sess = ort.InferenceSession(model.onnxModel.SerializeToString())

    modelInputs = {x.name: y for x, y in zip(model.sess.get_inputs(), inputs)}
    modelOutput = model.sess.run(None, modelInputs)

    mappedOutputs = {x.name: y for x, y in zip(model.sess.get_outputs(), modelOutput)}
    outputs = [None] * len(model.operations)
    for index, op in enumerate(model.operations):
        outputs[index] = mappedOutputs[op.output]

    return ModelRunResult(outputs)


class CDataEmitter:
    def __init__(self):
        self.arrays = []
        self.namedArrays = []

    def EmitNamedArray(self, name, dtype, data):
        self.namedArrays.append(TypedArray(dtype, copy(data), name))

    def EmitArray(self, dtype, data):
        assert isinstance(data, list)
        index = len(self.arrays)

        self.arrays.append(TypedArray(dtype, copy(data)))

        return CDataHandle(index)

    def Representation(self):
        content = ""

        def ItemRepr(item):
            if isinstance(item, CDataHandle):
                return f"temp_{item.index}"
            elif isinstance(item, list):
                return "{" + ",".join([ItemRepr(x) for x in item]) + "}"
            else:
                return str(item)

        for index, tarray in enumerate(self.arrays):
            dtype = tarray.dtype
            data = tarray.data

            content += (
                f"{dtype} temp_{index}[] = "
                + "{"
                + ",".join(ItemRepr(x) for x in data)
                + "};\n"
            )

        for index, tarray in enumerate(self.namedArrays):
            dtype = tarray.dtype
            data = tarray.data
            name = tarray.name
            amount = len(tarray.data)

            content += (
                f"{dtype} {name}[{amount}] = "
                + "{"
                + ",".join(ItemRepr(x) for x in data)
                + "};\n"
            )

        return content


# Copied from onnxruntime/tools/python/remove_initializer_from_input.py
def remove_initializer_from_input(model: onnx.ModelProto) -> bool:
    if model.ir_version < 4:
        print(
            "Model with ir_version below 4 requires to include initializer in graph input"
        )
        return False

    inputs = model.graph.input
    name_to_input = {}
    for input in inputs:
        name_to_input[input.name] = input

    modified = False
    for initializer in model.graph.initializer:
        if initializer.name in name_to_input:
            modified = True
            inputs.remove(name_to_input[initializer.name])

    return modified


def GenerateDebug(
    testLocation: str, modelName: str, binOutputLocation: str, sourceOutputLocation: str
):
    print(
        f"onnx.__version__={__version__!r}, opset={onnx_opset_version()}, IR_VERSION={IR_VERSION}"
    )

    # TODO: Only fetching one test, mainly because only a few of the test models actually contain more than 1 test.
    amountOfTests = len(glob.glob(os.path.join(testLocation, "test_data_set_*")))
    print("Tests found", amountOfTests)
    testDataDir = os.path.join(testLocation, "test_data_set_0")
    testModelLocation = os.path.join(testLocation, modelName)

    model = onnx.load(testModelLocation)
    remove_initializer_from_input(model)
    model = AddOutputsToEachNode(model)
    onnx.checker.check_model(model)

    # Perform inference
    sess = ort.InferenceSession(model.SerializeToString())

    inputs = []
    inputs_num = len(glob.glob(os.path.join(testDataDir, "input_*.pb")))
    for i in range(inputs_num):
        input_file = os.path.join(testDataDir, "input_{}.pb".format(i))
        tensor = onnx.TensorProto()
        with open(input_file, "rb") as f:
            tensor.ParseFromString(f.read())
        inputs.append(numpy_helper.to_array(tensor))

    isIntermediate = [False] * len(sess.get_outputs())

    for index, output in enumerate(sess.get_outputs()):
        if "INTERMEDIATE" in output.name:
            isIntermediate[index] = True

    ref_outputs = []
    ref_outputs_num = len(glob.glob(os.path.join(testDataDir, "output_*.pb")))
    for i in range(ref_outputs_num):
        output_file = os.path.join(testDataDir, "output_{}.pb".format(i))
        tensor = onnx.TensorProto()
        with open(output_file, "rb") as f:
            tensor.ParseFromString(f.read())
        ref_outputs.append(numpy_helper.to_array(tensor))

    modelInputs = {x.name: y for x, y in zip(sess.get_inputs(), inputs)}
    modelOutput = sess.run(None, modelInputs)

    properOutputs = []
    for index, output in enumerate(modelOutput):
        if not isIntermediate[index]:
            properOutputs.append(output)

    for ref_o, o in zip(ref_outputs, properOutputs):
        np.testing.assert_allclose(ref_o, o, rtol=1e-05)

    print("Test outputs match with the expected values")

    cModel = GenerateModelFromOnnxModel(model)

    # pprint(cModel)
    # sys.exit(0)

    # TODO: Implement multiple testcases by running the model multiple times and outputting multiple correct data bins.
    # NOTE: Is it possible for different testcases to generate different amounts of correctData? It shouldn't be possible.
    result = RunModel(cModel, inputs)
    correctData = result.outputs

    packedInputs = PackMultipleArrays(inputs)
    packedCorrectData = PackMultipleArrays(correctData)
    packedInitializers = PackMultipleArrays(cModel.initializers)

    with open(os.path.join(sourceOutputLocation, "code.c"), "w") as f:
        f.write('#include "versat_ai.h"\n')
        f.write('#include "stdint.h"\n')

        f.write(f"int numberLayers = {len(cModel.operations)};\n")

        layerInfo = []
        for index, c in enumerate(cModel.operations):
            outputSize = TensorSize(c.outputDimensions)
            layerInfo.append("{" + f'"{c.nodeName}","{c.opName}",{outputSize}' + "}")

        f.write("LayerInfo layers[] = {" + ",".join(layerInfo) + "};\n")

        opcodeToOperationList = {}
        for index, c in enumerate(cModel.operations):
            opcodeToOperationList[c.opName] = opcodeToOperationList.get(
                c.opName, []
            ) + [c]

        emitter = CDataEmitter()

        for opcode in opcodeToOperationList:
            if not IsOperatorRegistered(opcode):
                continue

            operationList = opcodeToOperationList[opcode]

            structs = []
            for index, op in enumerate(operationList):
                structs.append(EmitParameterList(emitter, op))

            emitter.EmitNamedArray(f"{opcode}Infos", f"{opcode}Info", structs)

        content = emitter.Representation()
        f.write(content)

        # Placeholder for operators not yet registered
        for opcode, opList in opcodeToOperationList.items():
            amount = len(opList)

            if IsOperatorRegistered(opcode):
                continue

            content = []
            for op in opList:
                content.append("{}")

            f.write(
                f"{opcode}Info {opcode}Infos[{amount}] = "
                + "{"
                + ",".join(content)
                + "};\n"
            )

        f.write("\n")
        f.write(
            "InferenceOutput DebugRunInference(void* outputMemory,void* temporaryMemory,void** inputs,void* modelMemory,void* correctInput){\n"
        )

        debugging = True
        opSeen = {}
        for index, c in enumerate(cModel.operations):
            content = []
            for inp in c.inputs:
                if inp.sourceType == DataSourceType.INITIALIZER:
                    content.append(
                        f"OFFSET_PTR(modelMemory,{packedInitializers.offsets[inp.index]})"
                    )
                elif inp.sourceType == DataSourceType.MODEL_INPUT:
                    content.append(f"inputs[{inp.index}]")
                else:
                    if debugging:
                        content.append(
                            f"OFFSET_PTR(correctInput,{packedCorrectData.offsets[inp.index]})"
                        )
                    else:
                        content.append(f"res_{inp.index}")

            outputStr = ""
            if c.outputMemoryAddress.memType == MemoryType.TEMP:
                outputStr = (
                    f"OFFSET_PTR(temporaryMemory,{c.outputMemoryAddress.offset})"
                )
            else:
                outputStr = f"OFFSET_PTR(outputMemory,{c.outputMemoryAddress.offset})"

            opIndex = opSeen.get(c.opName, -1) + 1
            opSeen[c.opName] = opIndex

            # TODO: Just move spec to inside the operator and have the parse function initialize it.
            spec = GetOperatorSpec(c.opName)
            decider = "Software_" if not spec.generateVersatCode else "Versat_"

            opName = c.opName
            if opName == "Conv":
                if len(c.inputs) == 3:
                    opName = "ConvWithBias"

            f.write(
                f"  void* res_{index} = "
                + decider
                + opName
                + "("
                + ",".join(content)
                + f",{outputStr},{index},&{c.opName}Infos[{opIndex}]);\n"
            )
            if debugging and (IsOperatorRegistered(c.opName)):
                f.write(
                    f"  AssertAlmostEqual(res_{index},OFFSET_PTR(correctInput,{packedCorrectData.offsets[index]}),{index});\n"
                )
        f.write("  return (InferenceOutput){};\n")
        f.write("}\n")

    # pprint([x.inputDimensions for x in cModel.operations])
    # sys.exit(0)
    with open(os.path.join(sourceOutputLocation, "modelInfo.h"), "w") as f:
        f.write("#pragma once\n")
        f.write(f"#define VERSAT_AI_OUTPUT_SIZE {cModel.outputMemoryNeeded}\n")
        f.write(f"#define VERSAT_AI_TEMP_SIZE {cModel.tempMemoryNeeded}\n")
        f.write(f"#define VERSAT_AI_MODEL_SIZE {len(packedInitializers.data)}\n")
        f.write(f"#define VERSAT_AI_CORRECT_SIZE {len(packedCorrectData.data)}\n")
        f.write(f"#define VERSAT_AI_N_INPUTS {len(cModel.modelInputs)}\n")

        inputSizes = [TensorSize(x.shape) for x in cModel.modelInputs]
        inputOffsets, totalInputSize = CalculateOffsetFromSize(inputSizes)

        for index, size in enumerate(inputSizes):
            f.write(f"#define VERSAT_AI_INPUT_{index}_SIZE {size}\n")

        f.write(f"#define VERSAT_AI_ALL_INPUTS_SIZE {totalInputSize}\n")

        f.write("const int VERSAT_AI_INPUT_OFFSET[] = {")
        f.write(",".join([str(x) for x in inputOffsets]))
        f.write("};\n")

    try:
        os.makedirs(binOutputLocation)
    except:
        pass

    with open(os.path.join(binOutputLocation, "inputs.bin"), "wb") as f:
        f.write(packedInputs.data)

    with open(os.path.join(binOutputLocation, "correctOutputs.bin"), "wb") as f:
        f.write(packedCorrectData.data)

    with open(os.path.join(binOutputLocation, "model.bin"), "wb") as f:
        f.write(packedInitializers.data)


if __name__ == "__main__":
    if len(sys.argv) < 5:
        print(
            "Error, script requires 4 parameters, <testLocation> <modelName> <binOutputLocation> <sourceOutputLocation>"
        )
        sys.exit(0)
    GenerateDebug(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])

# TODO: Need to take care with alignment issues. Embedded usually cannot handle misaligned data.

# TODO: Need to start giving NAMESPACE names to C stuff, this code is supposed to be easy to integrate anywhere.

# TODO: Need to also output the input file so we can start testing things and implementing the operators in C code.
#       Need to output parameters. Do not know the approach that we should take here.
#         Should we generate optimized functions for a given set of parameters?
#         Or should we just embed the data in source code and let runtime deal with it?
#         Or should we just pack it into a file and load at runtime? (Should only be a few kbs).
#       Even if we embed the data, I think it would be helpful to make a Conv2D especial function.
